{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d53f792-fb9a-4f3a-95f5-0986b1d1a615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.6 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./venv/lib/python3.13/site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in ./venv/lib/python3.13/site-packages (from seaborn) (2.2.3)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Downloading matplotlib-3.10.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading contourpy-1.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading fonttools-4.58.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.10.3-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp313-cp313-macosx_11_0_arm64.whl (255 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.0-cp313-cp313-macosx_10_13_universal2.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 pyparsing-3.2.3 seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.13/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.13/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in ./venv/lib/python3.13/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.13/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./venv/lib/python3.13/site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.15.3-cp313-cp313-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m897.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2a48c8fd-602b-4791-a9c4-fb03cc7055cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial statistics:\n",
      "                Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
      "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
      "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
      "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
      "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
      "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
      "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
      "max    150.000000       7.900000      4.400000       6.900000      2.500000\n",
      "\n",
      "Missing values:\n",
      " Id               0\n",
      "SepalLengthCm    0\n",
      "SepalWidthCm     0\n",
      "PetalLengthCm    0\n",
      "PetalWidthCm     0\n",
      "Species          0\n",
      "dtype: int64\n",
      "Id: 0 outliers\n",
      "SepalLengthCm: 0 outliers\n",
      "SepalWidthCm: 4 outliers\n",
      "PetalLengthCm: 0 outliers\n",
      "PetalWidthCm: 0 outliers\n",
      "\n",
      "Transformed data preview:\n",
      "    Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species  \\\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa   \n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa   \n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa   \n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa   \n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa   \n",
      "\n",
      "   PetalLengthCm_scaled  Species_encoded  \n",
      "0             -1.374878                0  \n",
      "1             -1.374878                0  \n",
      "2             -1.431986                0  \n",
      "3             -1.317771                0  \n",
      "4             -1.374878                0  \n"
     ]
    }
   ],
   "source": [
    "#Outliers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(\"Iris.csv\")  # Make sure this file is in your working directory\n",
    "\n",
    "# Step 2: Show initial statistics\n",
    "print(\"Initial statistics:\\n\", df.describe())\n",
    "\n",
    "# Step 3: Check for missing values\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Step 4: Detect outliers using IQR method\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "    print(f\"{col}: {len(outliers)} outliers\")\n",
    "\n",
    "# Optional: Remove outliers (example: for 'SepalWidthCm')\n",
    "Q1 = df['SepalWidthCm'].quantile(0.25)\n",
    "Q3 = df['SepalWidthCm'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[(df['SepalWidthCm'] >= Q1 - 1.5*IQR) & (df['SepalWidthCm'] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "# Step 5: Apply standard scaling to a numeric column\n",
    "scaler = StandardScaler()\n",
    "df['PetalLengthCm_scaled'] = scaler.fit_transform(df[['PetalLengthCm']])\n",
    "\n",
    "# Step 6: Convert categorical variable into numeric\n",
    "encoder = LabelEncoder()\n",
    "df['Species_encoded'] = encoder.fit_transform(df['Species'])\n",
    "\n",
    "# Show final DataFrame\n",
    "print(\"\\nTransformed data preview:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b4321809-6e56-412b-821f-cda83d77aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      RM  LSTAT  PTRATIO      MEDV\n",
      "0  6.575   4.98     15.3  504000.0\n",
      "1  6.421   9.14     17.8  453600.0\n",
      "2  7.185   4.03     17.8  728700.0\n",
      "3  6.998   2.94     18.7  701400.0\n",
      "4  7.147   5.33     18.7  760200.0\n",
      "[342593.79029768 506257.0916297  410499.93166174 237792.7411537\n",
      " 327005.79653234 403018.068531   261060.38389067 701308.47374597\n",
      " 362924.70496746 585818.82333754]\n",
      "Mean Squared Error: 0.21\n",
      "R-squared Score: -0.10\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error , r2_score\n",
    "# Load the dataset\n",
    "df = pd.read_csv('housing.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop('MEDV', axis=1)\n",
    "Y = df['MEDV']\n",
    "\n",
    "# Split into train and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "Y_predict = model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "# Show predictions (optional)\n",
    "print(Y_predict[:10])  # print first 10 predicted values\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1bd8ef-2526-4388-81ab-06ae6eb99d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(\"Iris.csv\")  # Make sure this file is in your working directory\n",
    "\n",
    "# Step 2: Show initial statistics\n",
    "print(\"Initial statistics:\\n\", df.describe())\n",
    "\n",
    "# Step 3: Check for missing values\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Step 4: Detect outliers using IQR method\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "    print(f\"{col}: {len(outliers)} outliers\")\n",
    "\n",
    "# Optional: Remove outliers (example: for 'SepalWidthCm')\n",
    "Q1 = df['SepalWidthCm'].quantile(0.25)\n",
    "Q3 = df['SepalWidthCm'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[(df['SepalWidthCm'] >= Q1 - 1.5*IQR) & (df['SepalWidthCm'] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "# Step 5: Apply standard scaling to a numeric column\n",
    "scaler = StandardScaler()\n",
    "df['PetalLengthCm_scaled'] = scaler.fit_transform(df[['PetalLengthCm']])\n",
    "\n",
    "# Step 6: Convert categorical variable into numeric\n",
    "encoder = LabelEncoder()\n",
    "df['Species_encoded'] = encoder.fit_transform(df['Species'])\n",
    "\n",
    "# Show final DataFrame\n",
    "print(\"\\nTransformed data preview:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78981d33-d91c-40a2-ad94-bd656890e267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "29c64fdd-c723-4886-9c31-c84bb257746f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (80, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m y_train = sc.transform(x_test)\n\u001b[32m     20\u001b[39m model = LogisticRegression()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m y_pred = model.predict(x_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DSBD Project/practicals/venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DSBD Project/practicals/venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DSBD Project/practicals/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DSBD Project/practicals/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1387\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m   1370\u001b[39m X = check_array(\n\u001b[32m   1371\u001b[39m     X,\n\u001b[32m   1372\u001b[39m     accept_sparse=accept_sparse,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1384\u001b[39m     input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1385\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1387\u001b[39m y = \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DSBD Project/practicals/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1408\u001b[39m, in \u001b[36m_check_y\u001b[39m\u001b[34m(y, multi_output, y_numeric, estimator)\u001b[39m\n\u001b[32m   1406\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1407\u001b[39m     estimator_name = _check_estimator_name(estimator)\n\u001b[32m-> \u001b[39m\u001b[32m1408\u001b[39m     y = \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1409\u001b[39m     _assert_all_finite(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, estimator_name=estimator_name)\n\u001b[32m   1410\u001b[39m     _ensure_no_complex_data(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DSBD Project/practicals/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1485\u001b[39m, in \u001b[36mcolumn_or_1d\u001b[39m\u001b[34m(y, dtype, warn, device)\u001b[39m\n\u001b[32m   1472\u001b[39m         warnings.warn(\n\u001b[32m   1473\u001b[39m             (\n\u001b[32m   1474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mA column-vector y was passed when a 1d array was\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1479\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1480\u001b[39m         )\n\u001b[32m   1481\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(\n\u001b[32m   1482\u001b[39m         xp.reshape(y, (-\u001b[32m1\u001b[39m,)), order=\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m, xp=xp, device=device\n\u001b[32m   1483\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1486\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m.format(shape)\n\u001b[32m   1487\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: y should be a 1d array, got an array of shape (80, 2) instead."
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "df = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = df[['Age' , 'EstimatedSalary']]\n",
    "Y = df['Purchased']\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(X , Y , test_size = 0.2 , random_state = 42)\n",
    "\n",
    "#feature scaling\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "y_train = sc.transform(x_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train , x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e596de-1bcb-4586-8486-5d305589fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix , accuracy_score , precision_score , recall_score \n",
    "\n",
    "df = pd.read_csv('Iris.csv')\n",
    "le = LabelEncoder()\n",
    "df['species'] = le.fit_transform(df['species'])\n",
    "\n",
    "x = df.drop('species' , axis=1)\n",
    "y = df['species']\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42)\n",
    "model = KNeighborsClassifier(n_neighbors = 3)\n",
    "model.fit(x_train , y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "cm = confusion_matrix(y_test , y_predict)\n",
    "print(\"Confusion Matrix : \" , cm)\n",
    "accuracy = accuracy_score(y_test , y_predict)\n",
    "error_rate = 1 - accuracy\n",
    "precision = precision_score(y_test , y_predict ,  average = 'macro')\n",
    "recall = recall_score(y_test , y_pred , average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f590e21e-d4ac-48b8-b227-14fb1a1c3f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[145   2]\n",
      " [ 41  12]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87       147\n",
      "           1       0.86      0.23      0.36        53\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.82      0.61      0.61       200\n",
      "weighted avg       0.80      0.79      0.74       200\n",
      "\n",
      "Accuracy Score: 0.785\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('emails.csv')\n",
    "\n",
    "# Drop the 'Email No.' column if present\n",
    "df = df.drop(columns=['Email No.'])\n",
    "\n",
    "# Features and Target\n",
    "X = df.drop(columns=['Prediction'])\n",
    "y = df['Prediction']\n",
    "\n",
    "# Reduce to 1000 samples for performance (optional)\n",
    "X = X.head(1000)\n",
    "y = y.head(1000)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVM model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908d15a-07a3-4044-9544-6096b7b8ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data     # Features\n",
    "y = iris.target   # Labels (species)\n",
    "\n",
    "# Step 2: Standardize the features (PCA works better when data is scaled)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Apply PCA (reduce to 2 principal components for visualization)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 4: Plot the results\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='viridis', edgecolor='k', s=70)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA on Iris Dataset')\n",
    "plt.grid(True)\n",
    "plt.colorbar(label='Target Classes')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Print explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
